{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISpahE2pv-Sn"
   },
   "source": [
    "## Install Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uojysf3v5XJ"
   },
   "outputs": [],
   "source": [
    "#packages have to be installed in requirements.txt before running this colab notebook.\n",
    "#Importing essential packages.\n",
    "\n",
    "#https://www.tensorflow.org/install/pip\n",
    "import tensorflow as tf\n",
    "#https://numpy.org/doc/stable/user/absolute_beginners.html\n",
    "import numpy as np\n",
    "#https://docs.python.org/3/library/unicodedata.html\n",
    "import unicodedata\n",
    "#https://docs.python.org/3/library/re.html\n",
    "import re\n",
    "#https://docs.python.org/3/library/time.html\n",
    "import time\n",
    "#https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mjWYEdDwFdS"
   },
   "source": [
    "## Import packages and define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZgHaIdIwB_n"
   },
   "outputs": [],
   "source": [
    "#we had took the dataset from \" https://www.kaggle.com/sunnysai12345/news-summary \"\n",
    "data_set=pd.read_csv(\"news_summary_more.csv\",nrows=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcY5ZNIBxLkW"
   },
   "outputs": [],
   "source": [
    "#we are implementing our data to 25 only for computation limitations.\n",
    "raw_data=data_set['text'].head(25)\n",
    "summary_data=data_set['headlines'].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Akef33fwWoY"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLnDmjq3wW3K"
   },
   "outputs": [],
   "source": [
    "#This function \"convert_uni_2_ascii\" convert various unicode data based on their ascii values.\n",
    "def convert_uni_2_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "#noramlizing the string (preprocessing data).\n",
    "def normalize(s):\n",
    "    s = convert_uni_2_ascii(s)\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s\n",
    "\n",
    "#Assigning raw_data and summarized data in data_set into lists.\n",
    "raw_data, summary_data = list(raw_data), list(summary_data)\n",
    "#applying pre processing over the data.\n",
    "raw_data = [normalize(data) for data in raw_data]\n",
    "#modifying the data with \"<start>\" and \"<end>\" quotes to feed the network.\n",
    "raw_data_in = ['<start> ' + normalize(data) for data in summary_data]\n",
    "summary_data_out = [normalize(data) + ' <end>' for data in summary_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHV961YhwdT2"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8DiLHXEwdaj"
   },
   "outputs": [],
   "source": [
    "#Intializing the tensorflow tokenizer for raw_data.\n",
    "raw_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "#Fitting the data.\n",
    "raw_tokenizer.fit_on_texts(raw_data)\n",
    "data_raw = raw_tokenizer.texts_to_sequences(raw_data)\n",
    "#Applying padding to get all the data into same length vectors.\n",
    "data_raw = tf.keras.preprocessing.sequence.pad_sequences(data_raw,padding='post')\n",
    "#Intializing the tensorflow tokenizer for summary_data in and out.\n",
    "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "#Fitting the data.\n",
    "summary_tokenizer.fit_on_texts(raw_data_in)\n",
    "summary_tokenizer.fit_on_texts(summary_data_out)\n",
    "data_summ_in = summary_tokenizer.texts_to_sequences(raw_data_in)\n",
    "#Applying padding to get all the data into same length vectors.\n",
    "data_summ_in = tf.keras.preprocessing.sequence.pad_sequences(data_summ_in, padding='post')\n",
    "data_summ_out = summary_tokenizer.texts_to_sequences(summary_data_out)\n",
    "#Applying padding to get all the data into same length vectors.\n",
    "data_summ_out = tf.keras.preprocessing.sequence.pad_sequences(data_summ_out,padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9Unosn7wnia"
   },
   "source": [
    "## Create tf.data.Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iezeyXYIwp38"
   },
   "outputs": [],
   "source": [
    "#Slicing the data into batches to feed into the network.\n",
    "size = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_raw, data_summ_in, data_summ_out))\n",
    "#we are shuffle rate to 10 because we had limited our data.\n",
    "dataset = dataset.shuffle(10).batch(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crchOTQR-6yo"
   },
   "source": [
    "## Create the Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mmdYJCA_Bfo"
   },
   "outputs": [],
   "source": [
    "#Defining the basic block of ATTENTION mechanism positional embedding.\n",
    "#https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model\n",
    "def positional_embedding(pos, model_size):\n",
    "    #creating numpy array to store the values.\n",
    "    Pos_E = np.zeros((1, model_size))\n",
    "    for i in range(model_size):\n",
    "        if i % 2 == 0:\n",
    "            Pos_E[:, i] = np.sin(pos / 10000 ** (i / model_size))\n",
    "        else:\n",
    "            Pos_E[:, i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n",
    "    return Pos_E\n",
    "#assigning the parameters.\n",
    "max_length = max(len(data_raw[0]), len(data_summ_in[0]))\n",
    "MODEL_SIZE = 128\n",
    "\n",
    "pes = []\n",
    "for i in range(max_length):\n",
    "    pes.append(positional_embedding(i, MODEL_SIZE))\n",
    "\n",
    "pes = np.concatenate(pes, axis=0)\n",
    "pes = tf.constant(pes, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQXPZXQnwv4i"
   },
   "outputs": [],
   "source": [
    "#In this class CrossHeadAttention we are implementing cross head attention.\n",
    "#https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853\n",
    "class CrossHeadAttention(tf.keras.Model):\n",
    "    def __init__(self, model_size, h):\n",
    "      #calling main class.\n",
    "        super(CrossHeadAttention, self).__init__()\n",
    "        self.query_size = model_size \n",
    "        self.key_size = model_size \n",
    "        self.value_size = model_size \n",
    "        self.h = h\n",
    "        self.wq = [tf.keras.layers.Dense(self.query_size) for _ in range(h)]\n",
    "        self.wk = [tf.keras.layers.Dense(self.key_size) for _ in range(h)]\n",
    "        self.wv = [tf.keras.layers.Dense(self.value_size) for _ in range(h)]\n",
    "        self.wo = tf.keras.layers.Dense(model_size)\n",
    "\n",
    "    def call(self, decoder_output, encoder_output):\n",
    "        #Intializing heads lists to store the weights(parameters) of attention.\n",
    "        heads = []\n",
    "        for i in range(self.h):\n",
    "            score = tf.matmul(self.wq[i](decoder_output), self.wk[i](encoder_output), transpose_b=True) / tf.math.sqrt(tf.dtypes.cast(self.key_size, tf.float32))\n",
    "            alignment = tf.nn.softmax(score, axis=2)\n",
    "            head = tf.matmul(alignment, self.wv[i](encoder_output))\n",
    "            heads.append(head)\n",
    "        heads = tf.concat(heads, axis=2)\n",
    "        heads = self.wo(heads)\n",
    "        return heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC2RziziCT2G"
   },
   "source": [
    "## Create the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpaZWuuSCVYz"
   },
   "outputs": [],
   "source": [
    "#In this class Encoder we are implementing encoder mechanism.\n",
    "#https://jalammar.github.io/illustrated-transformer/\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
    "      #calling main class.\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model_size = model_size\n",
    "        self.num_layers = num_layers\n",
    "        self.h = h\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
    "        self.attention = [CrossHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
    "        self.dense_1 = [tf.keras.layers.Dense(512, activation='relu') for _ in range(num_layers)]\n",
    "        self.dense_2 = [tf.keras.layers.Dense(model_size) for _ in range(num_layers)]\n",
    "        self.ffn_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self, sequence):\n",
    "    #Intializing sub_in lists to store the weights(parameters) of encoder.\n",
    "        sub_in = []\n",
    "        for i in range(sequence.shape[1]):\n",
    "            embed = self.embedding(tf.expand_dims(sequence[:, i], axis=1))\n",
    "            sub_in.append(embed + pes[i, :])   \n",
    "        sub_in = tf.concat(sub_in, axis=1)\n",
    "        for i in range(self.num_layers):\n",
    "            sub_out = []\n",
    "            for j in range(sub_in.shape[1]):\n",
    "                attention = self.attention[i](\n",
    "                    tf.expand_dims(sub_in[:, j, :], axis=1), sub_in)\n",
    "                sub_out.append(attention)\n",
    "            sub_out = tf.concat(sub_out, axis=1)\n",
    "            sub_out = sub_in + sub_out\n",
    "            sub_out = self.attention_norm[i](sub_out)\n",
    "            ffn_in = sub_out\n",
    "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
    "            ffn_out = ffn_in + ffn_out\n",
    "            ffn_out = self.ffn_norm[i](ffn_out)\n",
    "            sub_in = ffn_out\n",
    "            \n",
    "        return ffn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P09CP8ehuuzh"
   },
   "outputs": [],
   "source": [
    "#In this class Decoder we are implementing decoder mechanism.\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.model_size = model_size\n",
    "        self.num_layers = num_layers\n",
    "        self.h = h\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
    "        self.attention_bot = [CrossHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_bot_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
    "        self.attention_mid = [CrossHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_mid_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)] \n",
    "        self.dense_1 = [tf.keras.layers.Dense(512, activation='relu') for _ in range(num_layers)]\n",
    "        self.dense_2 = [tf.keras.layers.Dense(model_size) for _ in range(num_layers)]\n",
    "        self.ffn_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, sequence, encoder_output):\n",
    "    #Intializing embed_out lists to store the weights(parameters) of embedding and post embedding.\n",
    "        embed_out = []\n",
    "        for i in range(sequence.shape[1]):\n",
    "            embed = self.embedding(tf.expand_dims(sequence[:, i], axis=1))\n",
    "            embed_out.append(embed + pes[i, :])\n",
    "        embed_out = tf.concat(embed_out, axis=1)\n",
    "        bot_sub_in = embed_out        \n",
    "        for i in range(self.num_layers):\n",
    "            bot_sub_out = []\n",
    "            for j in range(bot_sub_in.shape[1]):\n",
    "                values = bot_sub_in[:, :j, :]\n",
    "                attention = self.attention_bot[i](\n",
    "                    tf.expand_dims(bot_sub_in[:, j, :], axis=1), values)\n",
    "                bot_sub_out.append(attention)\n",
    "            bot_sub_out = tf.concat(bot_sub_out, axis=1)\n",
    "            bot_sub_out = bot_sub_in + bot_sub_out\n",
    "            bot_sub_out = self.attention_bot_norm[i](bot_sub_out)\n",
    "            mid_sub_in = bot_sub_out\n",
    "            mid_sub_out = []\n",
    "            for j in range(mid_sub_in.shape[1]):\n",
    "                attention = self.attention_mid[i](\n",
    "                    tf.expand_dims(mid_sub_in[:, j, :], axis=1), encoder_output)\n",
    "                mid_sub_out.append(attention)\n",
    "            mid_sub_out = tf.concat(mid_sub_out, axis=1)\n",
    "            mid_sub_out = mid_sub_out + mid_sub_in\n",
    "            mid_sub_out = self.attention_mid_norm[i](mid_sub_out)\n",
    "            ffn_in = mid_sub_out\n",
    "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
    "            ffn_out = ffn_out + ffn_in\n",
    "            ffn_out = self.ffn_norm[i](ffn_out)\n",
    "            bot_sub_in = ffn_out\n",
    "        logits = self.dense(ffn_out)    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98hKI4JSwBoE"
   },
   "outputs": [],
   "source": [
    "#Using SparseCategoricalCrossentropy to train the model.\n",
    "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss_func(targets, logits):\n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
    "    return loss\n",
    "\n",
    "#Using adam optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oVlfpLawua0"
   },
   "outputs": [],
   "source": [
    "#predict function to get summized vector for query data.\n",
    "def predict(test_source_text=None):\n",
    "    if test_source_text is None:\n",
    "        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
    "    print(test_source_text)\n",
    "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
    "    print(test_source_seq)\n",
    "\n",
    "    en_output = encoder(tf.constant(test_source_seq))\n",
    "\n",
    "    de_input = tf.constant([[fr_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
    "\n",
    "    out_words = []\n",
    "\n",
    "    while True:\n",
    "        de_output = decoder(de_input, en_output)\n",
    "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
    "        out_words.append(fr_tokenizer.index_word[new_word.numpy()[0][0]])\n",
    "\n",
    "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
    "\n",
    "        if out_words[-1] == '<end>' or len(out_words) >= 14:\n",
    "            break\n",
    "\n",
    "    print(' '.join(out_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95KrszHJwyWR"
   },
   "outputs": [],
   "source": [
    "#Training step\n",
    "@tf.function\n",
    "def train_step(source_seq, target_seq_in, target_seq_out):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_output = encoder(source_seq)\n",
    "        decoder_output = decoder(target_seq_in, encoder_output)\n",
    "        loss = loss_func(target_seq_out, decoder_output)\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6QJ_j5Dw5bs",
    "outputId": "a460e50f-9a6b-4ed7-e115-f9a0108909c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss nan\n",
      "Epoch 2 Loss nan\n",
      "Epoch 3 Loss nan\n",
      "Epoch 4 Loss nan\n",
      "Epoch 5 Loss nan\n",
      "Epoch 6 Loss nan\n",
      "Epoch 7 Loss nan\n",
      "Epoch 8 Loss nan\n",
      "Epoch 9 Loss nan\n",
      "Epoch 10 Loss nan\n",
      "Average elapsed time: 9.91s\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "start_time = time.time()\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
    "        loss = train_step(source_seq, target_seq_in,\n",
    "                          target_seq_out)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(\n",
    "          e + 1, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "vb0DJT-4zsfO",
    "outputId": "b1558064-59e2-4345-e6d3-6fd028793f16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saurav Kant an alumnus of upGrad and IIIT B s PG Program in Machine learning and Artificial Intelligence was a Sr Systems Engineer at Infosys with almost years of work experience . The program and upGrad s degree career support helped him transition to a Data Scientist at Tech Mahindra with salary hike . upGrad s Online Power Learning has powered lakh careers .\n",
      "[[37, 46, 30, 9, 46, 5, 37, 48, 1, 4, 46, 30, 39, 5, 1, 30, 1]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-5c1b1b9f043e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest_sent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-1bbf26b38989>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(test_source_text)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mde_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnew_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mout_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mde_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "for test_sent in raw_data[:5]:\n",
    "    test_sequence = normalize(test_sent)\n",
    "    predict(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wq5mE3h7lN9N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of transformer_simple.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
