# Abstractive_Text_Summarizer_Using_Transformers_with_CrossHeadAttention.

An encoder decoder transformer with cross head attention instead of multi head attention model is implemented for astractive text(news) summarisation.

"news_summary_mre.csv" dataset is used to train the model

Transformers architecture:

![alt text](https://github.com/HareshSuvvari9/Abstractive_Text_Summarizer_Using_Transformers_with_CrossHeadAttention/blob/main/transformers.png?raw=true)

Cross Head Attention :

![alt text](https://github.com/HareshSuvvari9/Abstractive_Text_Summarizer_Using_Transformers_with_CrossHeadAttention/blob/main/cross%20head.png?raw=true)

